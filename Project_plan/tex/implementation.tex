The tasks listed in the project objectives chapter will be further divided into smaller subtasks here to give a more detailed desicription of the tasks and what are the steps to complete the planned tasks. 

\section{Project initialization and planning (basic)}

The task is to get started with the project by discussing the objectives of the project and different possible
methods that are used to achieve these objectives.

\section{Preliminary study of open-source video object detection and object tracking methods that could be applied in the project (basic)}

To automatically localize different objects in a scene, an automatic video object detection method is required.
Nowadays these methods are commonly deep learning-based methods with a plethora of different classes. The task is
to search for a method which would be most suitable for the task-at-hand by looking into the upsides and downsides
of each given model for our task.

Object tracking methods can also be utilized in the project. These methods are commonly based on mathematical 
algorithms and have been found to be quite reliable even for 360 video. However, the vast majority of these 
methods are only able to track one object at a time and this object needs to be manually selected in the initial
video frame which adds some manual work. It is possible to combine these methods together with automatic object
detectors to provide more reliable object detection by avoiding cases where a plain object detector could lose
track of an object for a few frames.

An important aspect is also to study how to automatically get the sound source activities of different objects.
This information should somehow be combined with the object detection information.

\section{Designing the scenes that are to be recorded using a 360 video camera and the integrated microphone array (basic)}

Most of the classes that are in pre-trained object detection models are irrelevant for our task. Hence, there is a
need to manually select which classes in the pre-trained model are required for the scenes that are to be 
recorded. The rest of the classes can then be combined into an `other' class label.

The scenes should be designed so that there are multiple sources of sound simultaneously. These objects should
be over two meters from the video camera. Thus, the basic assumption is that the elevation angles are limited.
Furthermore, sound sources should not be right next to each other to not make the scenes too complex.

\section{Recording the designed scenes (basic)}
After the recording location is selected, the recording of the videos starts. Since the aim is to determine 
the location and time of different sound sources in the same recording, we will cover different objects in the 
same video clips. These objects can be adults talking, a kid crying, a dog barking, a bird singing, and so on.

\section{Developing a set of python scripts for handling the video data frames (basic data handling methods for video) (basic)}

\section{Developing a set of python scripts for applying object detection for the video data frames (basic)}

\section{Developing a method for outputting the detected visual objects, their locations and the time interval of detection to be used together with spatial audio detection (basic)}

\section{Testing the developed video handling scripts with the recorded video data (basic)}

There has long been an indispensable need for object detection, recognition and tracking in computer vision
applications. YOLO is a method which is used to detect or track single or multiple objects that are in the
camera's field of view. YOLO came on the computer vision scene with the seminal 2015 paper by Joseph Redmon et al.
“You Only Look Once: Unified, Real-Time Object Detection,” and immediately got a lot of attention by fellow
computer vision researchers. In our project, YOLO will be modified such that it detects all of the necessary
objects that appear as groud truth sound sources in the recorded scenes of the project.

\section{Developing a method for outputting the detected sound events, their locations and the time interval of detection to be used together with spatial audio detection (basic)}

\section{Manually mapping visual object labels with the sound event labels to be used for automated annotation (basic)}

\section{Documenting (this includes the final report and the presentations) (basic)}

\section{Testing the method against manual annotation (basic)}

\section{More complicated scenes with more events and more sounds (advanced)}

\section{Fusion of audio and video (advanced)}