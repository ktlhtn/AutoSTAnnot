\section{Task definitions}

The tasks listed in the project objectives chapter will be further divided into smaller subtasks here to give a more detailed desicription of the tasks and what are the steps to complete the planned tasks. 

\subsection{Project initialization and planning (basic)}

The task is to get started with the project by discussing the objectives of the project and different possible
methods that are used to achieve these objectives.

\subsection{Preliminary study of open-source video object detection and object tracking methods that could be applied in the project (basic)}

To automatically localize different objects in a scene, an automatic video object detection method is required.
Nowadays these methods are commonly deep learning-based methods with a plethora of different classes. The task is
to search for a method which would be most suitable for the task-at-hand by looking into the upsides and downsides
of each given model for our task.

Object tracking methods can also be utilized in the project. These methods are commonly based on mathematical 
algorithms and have been found to be quite reliable even for 360 video. However, the vast majority of these 
methods are only able to track one object at a time and this object needs to be manually selected in the initial
video frame which adds some manual work. It is possible to combine these methods together with automatic object
detectors to provide more reliable object detection by avoiding cases where a plain object detector could lose
track of an object for a few frames.

An important aspect is also to study how to automatically get the sound source activities of different objects.
This information should somehow be combined with the object detection information.

\subsection{Designing the scenes that are to be recorded using a 360 video camera and the integrated microphone array (basic)}

Most of the classes that are in pre-trained object detection models are irrelevant for our task. Hence, there is a
need to manually select which classes in the pre-trained model are required for the scenes that are to be 
recorded. The rest of the classes can then be combined into an `other' class label.

The scenes should be designed so that there are multiple sources of sound simultaneously. These objects should
be over two meters from the video camera. Thus, the basic assumption is that the elevation angles are limited.
Furthermore, sound sources should not be right next to each other to not make the scenes too complex.

\subsection{Recording the designed scenes (basic)}

After the recording locations are selected, the recording of the videos starts. Since the aim is to determine 
the location and time of occurrence for different sound sources in the same recording, we will cover different
objects in the same video clips. These objects can be e.g. adults talking, a kid crying, a dog barking, a bird
singing, and so on.

\subsection{Developing a set of Python scripts for handling the video data frames (basic data handling methods for video) (basic)}

The 360-video data frames need to be processed before they can be further used with object detection and
tracking methods. These will be most probably implemented with a set of Python scripts, although the use of Matlab
is possible as well.

\subsection{Developing a set of Python scripts for applying object detection for the video data frames (basic)}

By modifying and possibly combining already existing state-of-the-art tools for object detection and tracking, a
set of Python scripts will be implemented in order to apply these methods for the recorded data. Although the use
of Matlab is possible as well, the use of Python is convenient since it is an open-source programming language for
which the majority of recent state-of-the-art tools have been implemented in signal processing and machine 
learning tasks.

One of the most prominent object detection methods nowadays is YOLO, which is a method which is used to detect or
track single or multiple objects that are in the camera's field of view. YOLO came into the computer vision scene
with the seminal 2015 paper by Joseph Redmon et al. “You Only Look Once: Unified, Real-Time Object Detection,”
and immediately got a lot of attention by fellow computer vision researchers. In our project, the latest version
of YOLO (YOLOv4) will be modified such that it detects all of the necessary objects that appear as ground truth
sound sources in the recorded scenes of the project. Since YOLO is not originally designed for 360-video, a
mapping needs to be defined for the 360 videos in order to be processed by YOLO.

\subsection{Developing a method for outputting the detected visual objects, their locations and the time interval of detection to be used together with spatial audio detection (basic)}

A method for outputting the detected visual objects needs to be implemented. Also, the locations and the time
intervals of detection should be used together with spatial audio detection.

\subsection{Testing the developed video handling scripts with the recorded video data (basic)}

After proper video processing methods have been established, these methods will be tested for the recorded 360-video data to verify their real-life performance.

\subsection{Developing a method for outputting the detected sound events, their locations and the time interval of detection (basic)}

In addition to processing the video data, the spatial audio data needs to be processed. A method for outputting
the detected sound events and their locations should be developed. Furthermore, the time intervals of detection
should be created in order to be used together with the information of detected sound events and their locations.
An important question is that how to get the audio activities for each class.

\subsection{Manually mapping visual object labels with the sound event labels to be used for automated annotation (basic)}

The visual object labels need to be mapped to the sound event labels in order to be used in automated annotation.
Since the visual the methods that provide the object labels and sound event labels probably output different
kinds of labels, a mapping between these labels needs to be defined manually. For example, the method for audio
event labels might only provide a `vehicle' label since audio classification between different vehicles is 
difficult, but on the other hand the method for visual object labels might give labels such as `car', `truck' and
`taxi' since it is easier to differentiate these kinds of objects visually. In this example case, all of the 
three labels given by the visual object detector would be mapped to the common label `vehicle' provided by the
audio event classifier.

\subsection{Documenting (basic)}

The present project plan, the final report, and the respective presentations for both are the documentations
that are provided from this project.

\subsection{Testing the method against manual annotation (basic)}

The sole purpose of the project is to investigate if a method for reducing manual labour related to data annotation is possible to develop by combining video and audio detection models. Therefore the developed method needs to be tested against real manual annotation. The method is considered
inapplicable for real-life purposes if it is too laborious or inaccurate compared to manual
annotation.

\subsection{More complicated scenes with more events and more sounds (advanced)}

The first task of the advanced goal is to record more complicated scenes. Compared to the previous recordings, 
these scenes consist of more sound event classes and more classes simultaneously. Furthermore, there can also be
sound events which are closer to each other than they were in the previous recordings.

\subsection{Fusion of audio and video (advanced)}

The events that occur in the recordings are not independent of each other, but instead the audio and visual data
contains lots of correlation. Instead of processing the audio and video data separately, this correlation should
be utilized somehow in order to provide more reliable annotations.



\section{Resource allocations to tasks}

All of the tasks will be divided equally with the group members as agreed in the project-specific Telegram 
channel. The clients will provide the tools for creating the recordings. In addition, readymade implementations
that can be used as the basis for many of the different tasks are available online.

\section{Schedule for each task}

The work will be carried out as planned with the clients in each of the weekly remote meetings. In these meetings
the clients are informed about what has been done in the past week by the group members, and new goals for the
following week are set by the clients.