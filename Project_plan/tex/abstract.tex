Audio streams, such as broadcast news, documentaries and personal videos contain sounds from a wide variety of sound sources. These streams include detectable audio events of interest, such as speech, music or sounds of animals. Beside the detection of sound event presence there is a need for detecting the direction of arrival (DOA) of the detected sound event (e.g., “adult talking” from 1 sec to 5 sec with a 30 degree azimuth and 20 degree elevation angles). For such cases deep learning methods are often used, as they usually produce the best results. However, with all data intensive modelling methods vast amounts of labeled training data is required for training the model. Manual annotation of audio events is laborious and difficult to do reliably, especially if the listener doing the annotation also needs to recognize the DOA for the sound events. So far simulated data has been used to avert this problem, but real world recordings would be preferred. The purpose of this project is to investigate the possibility of automatically annotating spatial and temporal dimensions of sound objects detected in an audiostream with the help of a 360-degree video recording of the same scene from which both the direction and presence of visual objects could be inferred and combined with temporal audio detection. The project was given as an assignment by Archontis Politis and Tuomas Virtanen from the audio research group at Tampere University. The goal of the project is to research if an automatical annotation system could be developed and if so, to create a proof of concept application from which the development can be further continued. The project will be organized as follows:

\begin{itemize}
	\item Studying object detection methods for 360-degree video data
	\item Determining and defining the scenes to be recorded
	\item Recording spatiotemporal scenes with a 360-degree camera and integrated microphone array
	\item Applying object detection to the video recording 
	\item Extracting temporal object detections and their locations from the video recording
	\item Formulating spatiotemporal labels for the audiodata by combining video and audio detections
	\item Testing the method against manual annotation
\end{itemize}