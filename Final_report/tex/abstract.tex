This report describes the goals, work and results of the Signal Processing Innovation Project course implemented in the spring of 2021 at Tampere University.

Annotating spatio-temporal audio data for machine learning based models is considered a laborious task with a lot of uncertainties especially in the evaluation of the spatial dimension. The goal of the project was to research the possibility of using visual detections from 360-degree video recordings in automatically creating annotated training data for audio model training. The project consisted of researching different detection and tracking methods available for 360-video data, learning about spatial audio processing methods and recording test scenes with a 360-video camera and a microphone array. Two different data processing pipelines combining the visual detections made from the video recordings with the audio recording are proposed. The proposed pipelines are based on framewise video detections from four stereographic subprojections using the YOLOv4 object detector and either audio powermap processing or audio beamforming for infering audio activity. Both of these pipelines provide training data with sound source location and activity included as an output.

