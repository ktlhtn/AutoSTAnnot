This project is the course work for a Tampere University course 'Signal Processing Innovation Project' designed for students majoring in signal processing and machine learning. The course will be completed during spring of the year 2021. The students working on the project are Einari Vaaras, Mehmet Aydin and Kalle Lahtinen. The goal of the project is to study the possibilities for automatically annotating spatial and temporal dimensions of sound objects detected in an audiostream with the help of a 360-degree video recording. The premise is that the objects are over two meters away from the the camera, so the project involves limited elevation angles. Another basic element is that including multiple sound sources is relevant, but there is no need to go to extremes, such as two similar objects being very close to each other.

The need for such a tool rises from the research of spatial audio in which data intensive modelling methods are heavily used. The goal of the project is to provide a proof-of-concept application which would improve the quality of spatial audio data used in the training of such models and reduce manual labour related to the annotation of the data. The project will be implemented using open-source tools. The main tool for development will be Python and available signal processing and machine learning libraries. The resulting application and its source code will be shared as open-source for further research use.